{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality Checks for the Translated BeaverTails Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook performs a series of quality assurance checks on the translated version of the BeaverTails dataset.\n",
    "\n",
    "Quality checks:\n",
    "- Length ratio\n",
    "- Token ratio\n",
    "- Question preserved\n",
    "- Exclamation preserved\n",
    "- Truncated translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "\n",
    "from datasets import Dataset, load_dataset\n",
    "from pandas import DataFrame\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from src.evaluation.checks import exclamation_preserved, length_ratio, question_preserved, token_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_MAX_TOKENS = 512\n",
    "SEPARATOR = \"-\" * 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokens for original and translated examples\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"haoranxu/X-ALMA-13B-Group2\")\n",
    "\n",
    "\n",
    "def get_tokens(example: dict[str, str], src_field: str, mt_field: str) -> dict[str, str | list[str]]:\n",
    "    return {\n",
    "        \"src\": example[src_field],\n",
    "        \"mt\": example[mt_field],\n",
    "        \"src_tokens\": tokenizer.tokenize(example[src_field]),\n",
    "        \"mt_tokens\": tokenizer.tokenize(example[mt_field]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats(df: DataFrame):\n",
    "    # length and token ratios\n",
    "    df[\"length_ratio\"] = df.apply(lambda x: length_ratio(x[\"src\"], x[\"mt\"]), axis=1)\n",
    "    df[\"token_ratio\"] = df.apply(lambda x: token_ratio(x[\"src_tokens\"], x[\"mt_tokens\"]), axis=1)\n",
    "\n",
    "    # token lengths\n",
    "    df[\"src_token_len\"] = df.apply(lambda x: len(x[\"src_tokens\"]), axis=1)\n",
    "    df[\"mt_token_len\"] = df.apply(lambda x: len(x[\"mt_tokens\"]), axis=1)\n",
    "\n",
    "    # qestion marks\n",
    "    df[\"contains_question\"] = df.apply(lambda x: \"?\" in x[\"src\"], axis=1)\n",
    "    df[\"question_preserved\"] = df.apply(lambda x: question_preserved(x[\"src\"], x[\"mt\"]), axis=1)\n",
    "\n",
    "    # exclamation marks\n",
    "    df[\"contains_exclamation\"] = df.apply(lambda x: \"!\" in x[\"src\"], axis=1)\n",
    "    df[\"exclamation_preserved\"] = df.apply(lambda x: exclamation_preserved(x[\"src\"], x[\"mt\"]), axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_truncation(df: DataFrame, new_max_tokens: int, field: str = \"prompt\"):\n",
    "    truncated = df[df[\"mt_token_len\"] >= new_max_tokens]\n",
    "    print(f\"Number of truncated {field} translations: \", len(truncated))\n",
    "\n",
    "    if len(truncated) > 0:\n",
    "        display(truncated[[\"src\", \"mt\", \"mt_token_len\"]].sort_values(\"mt_token_len\", ascending=False))\n",
    "        print()\n",
    "\n",
    "\n",
    "def check_questions(df: DataFrame, field: str = \"prompt\"):\n",
    "    not_preserved = df[df[\"contains_question\"] & ~df[\"question_preserved\"]]\n",
    "    print(f\"Number of {field}s with question but not preserved: \", len(not_preserved))\n",
    "\n",
    "    if len(not_preserved) > 0:\n",
    "        display(not_preserved)\n",
    "        print()\n",
    "\n",
    "\n",
    "def check_exclamations(df: DataFrame, field: str = \"prompt\"):\n",
    "    not_preserved = df[df[\"contains_exclamation\"] & ~df[\"exclamation_preserved\"]]\n",
    "    print(f\"Number of {field}s with exclamation but not preserved: \", len(not_preserved))\n",
    "\n",
    "    if len(not_preserved) > 0:\n",
    "        display(not_preserved)\n",
    "        print()\n",
    "\n",
    "\n",
    "def display_length_ratio(df: DataFrame, sort_by: str = \"length_ratio\", ascending: bool = False, field: str = \"prompt\"):\n",
    "    df = df[[\"src\", \"mt\", \"src_token_len\", \"mt_token_len\", \"length_ratio\", \"token_ratio\"]].copy()\n",
    "\n",
    "    print(f\"{field.capitalize()} Length and Token Ratio Analysis\")\n",
    "    display(df.sort_values(sort_by, ascending=ascending))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PKU-Alignment/BeaverTails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and get tokens\n",
    "dataset = load_dataset(\"saiteki-kai/BeaverTails-it\", split=\"330k_test\")\n",
    "\n",
    "dataset_prompts = dataset.map(lambda example: get_tokens(example, \"prompt\", \"prompt_it\"))\n",
    "dataset_prompts = dataset_prompts.remove_columns([\"prompt\", \"prompt_it\", \"response\", \"response_it\"])\n",
    "dataset_prompts = typing.cast(Dataset, dataset_prompts)\n",
    "dataset_prompts_df = typing.cast(DataFrame, dataset_prompts.to_pandas())\n",
    "\n",
    "dataset_responses = dataset.map(lambda example: get_tokens(example, \"response\", \"response_it\"))\n",
    "dataset_responses = dataset_responses.remove_columns([\"prompt\", \"prompt_it\", \"response\", \"response_it\"])\n",
    "dataset_responses = typing.cast(Dataset, dataset_responses)\n",
    "dataset_responses_df = typing.cast(DataFrame, dataset_responses.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompts_df = compute_stats(dataset_prompts_df)\n",
    "dataset_responses_df = compute_stats(dataset_responses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_truncation(dataset_prompts_df, NEW_MAX_TOKENS, \"prompt\")\n",
    "check_truncation(dataset_responses_df, NEW_MAX_TOKENS, \"response\")\n",
    "print(SEPARATOR)\n",
    "\n",
    "check_questions(dataset_prompts_df, \"prompt\")\n",
    "check_questions(dataset_responses_df, \"response\")\n",
    "print(SEPARATOR)\n",
    "\n",
    "check_exclamations(dataset_prompts_df, \"prompt\")\n",
    "check_exclamations(dataset_responses_df, \"response\")\n",
    "print(SEPARATOR)\n",
    "\n",
    "display_length_ratio(dataset_prompts_df, \"length_ratio\", True, \"prompt\")\n",
    "display_length_ratio(dataset_responses_df, \"length_ratio\", True, \"response\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PKU-Alignment/BeaverTails-Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset and get tokens\n",
    "dataset = load_dataset(\"saiteki-kai/BeaverTails-it-Evaluation\", split=\"test\")\n",
    "\n",
    "dataset_prompts = dataset.map(lambda example: get_tokens(example, \"prompt\", \"prompt_it\"))\n",
    "dataset_prompts = typing.cast(Dataset, dataset_prompts)\n",
    "dataset_prompts_df = typing.cast(DataFrame, dataset_prompts.to_pandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompts_df = compute_stats(dataset_prompts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_truncation(dataset_prompts_df, NEW_MAX_TOKENS)\n",
    "print(SEPARATOR)\n",
    "\n",
    "check_questions(dataset_prompts_df)\n",
    "print(SEPARATOR)\n",
    "\n",
    "check_exclamations(dataset_prompts_df)\n",
    "print(SEPARATOR)\n",
    "\n",
    "display_length_ratio(dataset_prompts_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
